# -*- coding: utf-8 -*-
"""Customer_Segmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xCwvYMe9B4_Uq7x55GhPAUEWzNh_HWqg

####Importing all neccesary libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# StandardScaler for Feature Scalling
# LabelEncoder for Encoding categorical data
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans

"""####Loading the dataset"""

bank_data = pd.read_csv('/content/drive/MyDrive/Datasets /Bank Marketing Dataset/bank.csv')
bank_data.head()

"""##Data Cleaning and Processing

###Dropping the unwanted or unnecessary columns.

Some columns dont add values to the clustering so we will remove them. Columns like day and month.
"""

bank_data.drop(columns = ['day','month'], inplace=True)
bank_data.head()

"""###Convert Categorical Columns into Numbers

#### 1. Convert Yes/No values to 0/1. (Label Encoding)
"""

labels_col = ['default','housing','loan','deposit']

for col in labels_col:
  bank_data[col] = bank_data[col].map({ 'yes':1, 'no':0 })

bank_data[['default','housing','loan','deposit']].head()

"""#### 2.Convert Multi-Category Columns (One-Hot Encoding)

Some columns have multiple categories (like job, marital, education, etc.). We convert them using One-Hot Encoding.
"""

bank_data = pd.get_dummies(bank_data, columns=['job', 'marital', 'education', 'contact', 'poutcome'], drop_first = True)

bank_data.head()

"""### Feature Scaling (Standarization)

Kmeans works best when all the features are on the same scale. This will ensure that no column domintes due to large values.
"""

scaler = StandardScaler()
bankdata_scaled = scaler.fit_transform(bank_data)

"""###Finding the best K value by using Elbow Method

ðŸ”¹ 1. n_clusters=k â†’ Number of Groups

This sets how many clusters (K) we want.

In the loop, k changes from 1 to 10 to test different values of K.

Example:

If k=3, KMeans will try to divide the data into 3 groups.

If k=5, it will try to create 5 groups.

ðŸ”¹ 2. init='k-means++' â†’ Smart Initialization

Normally, K-Means randomly picks cluster centers, which can cause bad clustering.

'k-means++' improves this by:

Selecting the first centroid randomly.

Picking the next centroids far from existing ones, ensuring better cluster formation.

This helps reduce WCSS and makes clustering more efficient.

ðŸ”¹ 3. random_state=42 â†’ Reproducibility

K-Means uses randomness to select initial centroids.

random_state=42 makes sure that we get the same results every time we run the code.

If we remove random_state, the clusters may change each time we run the program.

*   kmeans.inertia_  -> Calculates the WCSS (Within-Cluster Sum of Squares) for the current k.
*   wcss.append(...) ->	Adds the calculated WCSS value to the wcss list.
"""

wcss = [] #within-cluster sum of squares
          # It will store the different values of wcss for cluster k.

for k in range (1,11):
  kmeans = KMeans(n_clusters=k,init='k-means++',random_state=42)
  kmeans.fit(bankdata_scaled)
  wcss.append(kmeans.inertia_)


#Lets make graph
plt.figure(figsize=(8, 5))
          #x-value     y-value
plt.plot(range(1, 11), wcss, marker='o', linestyle='--') #Draws a circle (o) at each data point for better visibility.
                                                         #Creates a dashed (--) line instead of a solid line.
plt.xlabel('Number of Clusters (K)')
plt.ylabel('WCSS')
plt.title('Elbow Method for Optimal K')
plt.show()

"""* It starts steep and then flattens.
* Elbow Point": The point where the decrease in WCSS slows down significantly.
* If we want a simpler clustering, K = 3 is a good choice.
* If we need more detailed clustering, K = 4 could be considered.

Lets consider k best value as 4.

###Applying KMeans Clustering
"""

kmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)
bank_data['Cluster'] = kmeans.fit_predict(bankdata_scaled)

"""* df['Cluster'] ->	Creates a new column in the df DataFrame to store the cluster labels.
* kmeans.fit_predict(df_scaled)	-> Runs K-Means clustering on df_scaled and returns the cluster each data point belongs to.
* fit_predict(df_scaled)	-> Fits the model on df_scaled (your scaled dataset).
- Predicts the cluster for each data point.

This assigns each customer to a cluster (0, 1, 2, or 3).

###Understanding the Clusters

####1) Check Cluster Distribution
"""

bank_data['Cluster'].value_counts()

"""This shows how many customers are in each cluster.

####2) See the Cluster Characteristics
"""

bank_data.groupby('Cluster').mean()

"""This helps us understand each clusterâ€™s financial behavior.

###Doing the findings using excel or spreadsheets.

First downloading the dataset with cluster column.
"""

bank_data.to_csv('clustered_data.csv', index=False)

from google.colab import files
files.download('clustered_data.csv')

"""###Visualizing the Clusters"""

plt.figure(figsize=(10, 6))
sns.scatterplot(x=bank_data['balance'], y=bank_data['age'], hue=bank_data['Cluster'], palette='viridis')
plt.title('Customer Clusters based on Age and Balance')
plt.show()